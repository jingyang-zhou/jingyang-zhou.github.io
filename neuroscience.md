---
layout: post
title: Neuroscience
---

## Representational dissimilarity metric spaces for stochastic neural networks

* Existing methods compare deterministic responses (e.g. artificial networks that lack stochastic layers) or averaged responses (e.g., trial-averaged firing rates in biological data). However, these measures of deterministic representational similarity ignore the scale and geometric structure of noise, both of which play important roles in neural computation. 

* We generalize previously proposed shape metrics (Williams et al., 2021) to quantify differences in stochastic representations. These new distances satisfy the triangle inequality, and thus can be used as a rigorous basis for many supervised and unsupervised analyses. 

* We find that the stochastic geometries of neurobiological representations of oriented visual gratings and naturalistic scenes respectively resemble untrained and trained deep network representations. Further, we are able to more accurately predict certain network attributes (e.g. training hyperparameters) from its position in stochastic (versus deterministic) shape space

## Delayed normalization model captures disparate nonlinear neural dynamics measured with different techniques in macaque and human V1

update


## Predicting neuronal dynamics with a delayed gain control model


## Systematic changes in temporal summation across human visual cortex
